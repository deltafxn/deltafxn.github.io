<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta http-equiv="X-UA-Compatible" content="IE=edge"/><title>doc6 · undefined</title><meta name="viewport" content="width=device-width"/><meta name="generator" content="Docusaurus"/><meta name="description" content="Now that we have some data labelled, let&#x27;s start using bootstrapping models to speed up our annotation. We will start by creating a new model by going to the &#x27;Models&#x27; tab in the project and clicking &#x27;Create model&#x27;"/><meta name="docsearch:language" content="en"/><meta property="og:title" content="doc6 · undefined"/><meta property="og:type" content="website"/><meta property="og:url" content="https://deltafxn.github.io/"/><meta property="og:description" content="Now that we have some data labelled, let&#x27;s start using bootstrapping models to speed up our annotation. We will start by creating a new model by going to the &#x27;Models&#x27; tab in the project and clicking &#x27;Create model&#x27;"/><meta property="og:image" content="https://deltafxn.github.io/img/undraw_online.svg"/><meta name="twitter:card" content="summary"/><meta name="twitter:image" content="https://deltafxn.github.io/img/undraw_tweetstorm.svg"/><link rel="shortcut icon" href="/img/favicon.ico"/><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css"/><script type="text/javascript" src="https://buttons.github.io/buttons.js"></script><script src="/js/scrollSpy.js"></script><link rel="stylesheet" href="/css/main.css"/><script src="/js/codetabs.js"></script></head><body class="sideNavVisible separateOnPageNav"><div class="fixedHeaderContainer"><div class="headerWrapper wrapper"><header><a href="/"><img class="logo" src="/img/favicon.ico"/><h2 class="headerTitleWithLogo"></h2></a><div class="navigationWrapper navigationSlider"><nav class="slidingNav"><ul class="nav-site nav-site-internal"></ul></nav></div></header></div></div><div class="navPusher singleRowMobileNav"><div class="docMainWrapper wrapper"><div class="container mainContainer docsContainer"><div class="wrapper"><div class="post"><header class="postHeader"><h1 id="__docusaurus" class="postHeaderTitle">doc6</h1></header><article><div><span><p>Now that we have some data labelled, let's start using bootstrapping models to speed up our annotation. We will start by creating a new model by going to the 'Models' tab in the project and clicking 'Create model'</p>
<p><img src="/img/model_create.png"></p>
<p>Let's a put a title and description on the model so we can keep track of it later. Now we can select options for instantiating this new model. The first and most important is what we want this model to do under 'Model type': &quot;Classification&quot;, &quot;Object detection&quot;, or &quot;Instance segmentation&quot;.</p>
<p><img src="/img/model_type.png"></p>
<p>We can also select the machine learning framework we want the model to use under 'Framework' and the specific model class under 'Model'.</p>
<p><img src="/img/model_architechture.png"></p>
<p>Finally, we want to select the classes or objects we want the model to register. For object detection and segmentation this is the specific objects to include. For classification it is which &quot;classification question&quot; we want the model to &quot;answer&quot;. This can be any question within the classification tree in your ontology.</p>
<p><img src="/img/model_objects.png"></p>
<p>We have now instantiated the model with what we want it to do and how he want it to do it. We can now go ahead and train the model with some labelled data. First, click on 'Train'</p>
<p><img src="/img/model_train_settings.png"></p>
<p>We can set the number of epochs we want the model to train for. Under 'Advanced Settings' we can additionally set the 'Batch size' if it is trained with a CPU or GPU(GPU recommended) and what set of training weights to initialize the model with.</p>
<p><img src="/img/model_train_advanced_settings.png"></p>
<p>Now we can select what data to include to train the model. Select the videos you want to use, click 'Train' and now let the learning begin!</p>
<p><img src="/img/model_train_data.png"></p>
<p>You can keep track of the progress of the model training by expanding the snack bar arrow at the bottom of the screen.</p>
<p><img src="/img/model_train_status.png"></p>
<p>Once the training is complete, the status of the model will update to say it is 'Trained'.</p>
<p><img src="/img/model_train_complete.png"></p>
<p>We can now use this trained model to help label more data. Let's go back into the editor and open the 'Automated Labelling' drawer. Because we just trained an object detection model, the trained model will now appear in the 'Object and Segmentation' list of available models.</p>
<p><img src="/img/inference_selection.png"></p>
<p>Once we select it we will get a new list of parameters we can set to run inference with. The main parameters we need to consider are the 'Confidence' and 'Detection range'. The confidence is the minimum model confidence required. By setting the confidence to a certain amount, the inference will not return any predictions below that specified confidence level. The detection range simply states which frames the inference will run over.</p>
<p><img src="/img/inference_settings.png"></p>
<p>We can expand 'Advanced Settings' and additionally set whether inference is run on CPU and GPU(GPU recommended) and the 'Intersection over union' threshold, which deletes boxes and polygons that have over that threshold with other boxes and polygons.</p>
<p><img src="/img/inference_advanced_settings.png"></p>
<p>Now we can run the inference and see the predictions return and get rendered in the editor. Note that we can also see the confidence of the predictions above the object boundary. We now use bulk operations setting to delete and boxes below a certain threshold or in a range of specific frames if we see the prediction hasn't done a good enough job.</p>
<p><img src="/img/inference_bulk.png"></p>
</span></div></article></div><div class="docs-prevnext"></div></div></div><nav class="onPageNav"></nav></div><footer class="nav-footer" id="footer"><section class="sitemap"><a href="/" class="nav-home"><img src="/img/favicon.ico" width="66" height="58"/></a><div><h5>Docs</h5><a href="/docs/doc1.html">Getting Started (or other categories)</a><a href="/docs/doc2.html">Guides (or other categories)</a><a href="/docs/doc3.html">API Reference (or other categories)</a></div><div><h5>Community</h5><a href="/users">User Showcase</a><a href="https://stackoverflow.com/questions/tagged/" target="_blank" rel="noreferrer noopener">Stack Overflow</a><a href="https://discordapp.com/">Project Chat</a><a href="https://twitter.com/" target="_blank" rel="noreferrer noopener">Twitter</a></div><div><h5>More</h5><a href="/blog">Blog</a><a href="https://github.com/">GitHub</a><a class="github-button" data-icon="octicon-star" data-count-href="/facebook/docusaurus/stargazers" data-show-count="true" data-count-aria-label="# stargazers on GitHub" aria-label="Star this project on GitHub">Star</a></div></section><a href="https://opensource.facebook.com/" target="_blank" rel="noreferrer noopener" class="fbOpenSource"><img src="/img/oss_logo.png" alt="Facebook Open Source" width="170" height="45"/></a><section class="copyright">Copyright © 2020 Your Name or Your Company Name</section></footer></div></body></html>